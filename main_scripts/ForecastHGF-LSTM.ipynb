{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b865df0f",
   "metadata": {},
   "source": [
    "### Family: Gender only (Indoors + Outdoors together)\n",
    "### Hierarchical LSTM:\n",
    "- Season Encoder LSTM over mark-level sequences -> season embedding\n",
    "- Add SeasonLabel embedding (Indoor/Outdoor) to each season embedding\n",
    "- Across-Season LSTM over K season embeddings -> athlete + family representation\n",
    "- Per-event heads (Linear) with per-event z-scored targets; report MAE in natural/native units\n",
    "### Sliding Windows:\n",
    "For each athlete & event_family, take up to K past seasons ending at season t (Indoor or Outdoor), and predict the peak in the next season with the SAME label (t+1, same SeasonLabel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8720882f",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d87ef1d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemError",
     "evalue": "<class 'numpy.iinfo'> returned a result with an exception set",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: partially initialized module 'pandas' from 'c:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\__init__.py' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mSystemError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\__init__.py:151\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcomputation\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28meval\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    135\u001b[39m     concat,\n\u001b[32m    136\u001b[39m     lreshape,\n\u001b[32m   (...)\u001b[39m\u001b[32m    148\u001b[39m     qcut,\n\u001b[32m    149\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m api, arrays, errors, io, plotting, tseries\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m testing\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_print_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\api\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\" public toolkit API \"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     extensions,\n\u001b[32m      4\u001b[39m     indexers,\n\u001b[32m      5\u001b[39m     interchange,\n\u001b[32m      6\u001b[39m     types,\n\u001b[32m      7\u001b[39m     typing,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m __all__ = [\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minterchange\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mextensions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtyping\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\api\\typing\\__init__.py:31\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwindow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     20\u001b[39m     Expanding,\n\u001b[32m     21\u001b[39m     ExpandingGroupby,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     Window,\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# TODO: Can't import Styler without importing jinja2\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# from pandas.io.formats.style import Styler\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_json\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JsonReader\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StataReader\n\u001b[32m     34\u001b[39m __all__ = [\n\u001b[32m     35\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDataFrameGroupBy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDatetimeIndexResamplerGroupby\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     54\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWindow\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     55\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\io\\json\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_json\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     read_json,\n\u001b[32m      3\u001b[39m     to_json,\n\u001b[32m      4\u001b[39m     ujson_dumps,\n\u001b[32m      5\u001b[39m     ujson_loads,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_table_schema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_table_schema\n\u001b[32m      9\u001b[39m __all__ = [\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mujson_dumps\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mujson_loads\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbuild_table_schema\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     15\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:71\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_normalize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_to_line_delimits\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mjson\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_table_schema\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     68\u001b[39m     build_table_schema,\n\u001b[32m     69\u001b[39m     parse_table_schema,\n\u001b[32m     70\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_integer\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mabc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     75\u001b[39m         Hashable,\n\u001b[32m     76\u001b[39m         Mapping,\n\u001b[32m     77\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     TextFileReader,\n\u001b[32m      3\u001b[39m     TextParser,\n\u001b[32m      4\u001b[39m     read_csv,\n\u001b[32m      5\u001b[39m     read_fwf,\n\u001b[32m      6\u001b[39m     read_table,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mTextFileReader\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mTextParser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mread_csv\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mread_fwf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mread_table\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:32\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m using_copy_on_write\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_libs\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparsers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m STR_NA_VALUES\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     34\u001b[39m     AbstractMethodError,\n\u001b[32m     35\u001b[39m     ParserWarning,\n\u001b[32m     36\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_decorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Appender\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2092\u001b[39m, in \u001b[36minit pandas._libs.parsers\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2067\u001b[39m, in \u001b[36mpandas._libs.parsers._compute_na_values\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mSystemError\u001b[39m: <class 'numpy.iinfo'> returned a result with an exception set"
     ]
    }
   ],
   "source": [
    "import argparse, os, re, json, random\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from types import SimpleNamespace\n",
    "\n",
    "\n",
    "NPY_PATH_DEFAULT = r\"C:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\tfrrs_performances_fast.ckpt_22000.npy\"\n",
    "OUT_DIR_DEFAULT = \"models_HierGenderFamilies\"\n",
    "CACHE_SUBDIR        = \"cache\"\n",
    "\n",
    "SEQLEN_MARKS = 32    # marks kept per season\n",
    "K_SEASONS = 4        # seasons kept in the window (history) → predict next season (same label)\n",
    "HID_SEASON = 64      # hidden size of season (mark-level) LSTM\n",
    "HID_ACROSS = 64      # hidden size of across-season LSTM\n",
    "EMB_LABEL = 4        # embedding dim for SeasonLabel (Indoor/Outdoor)\n",
    "BATCH = 128\n",
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "MIN_TRAIN_PER_EVENT = 60   # filter rare events inside each gender\n",
    "SEED = 42\n",
    "\n",
    "SEASONLABEL_ORDER = {\"Indoors\": 0, \"Outdoors\": 1}\n",
    "SEASONLABEL_ID = {\"Indoors\": 0, \"Outdoors\": 1}\n",
    "SEASONLABEL_FROM_ID = {0: \"Indoors\", 1: \"Outdoors\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8aaa20",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e87aaa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.manual_seed(seed)\n",
    "\n",
    "RUN_EVENTS = {\n",
    "    \"60 Meters\",\"100 Meters\",\"200 Meters\",\"400 Meters\",\"800 Meters\",\"1500 Meters\",\n",
    "    \"Mile\",\"3000 Meters\",\"5000 Meters\",\"10000 Meters\",\"60 Hurdles\",\"110 Hurdles\",\"100 Hurdles\",\n",
    "    \"400 Hurdles\",\"3000 Steeplechase\",\"DMR\",\"4 x 100 Relay\",\"4 x 400 Relay\"\n",
    "}\n",
    "FIELD_EVENTS = {\n",
    "    \"Long Jump\",\"Triple Jump\",\"High Jump\",\"Pole Vault\",\n",
    "    \"Shot Put\",\"Discus\",\"Hammer\",\"Javelin\",\"Weight Throw\"\n",
    "}\n",
    "\n",
    "def canonical_event(e: str) -> str:\n",
    "    if e in RUN_EVENTS or e in FIELD_EVENTS: return e\n",
    "    el = (e or \"\").strip().lower()\n",
    "    alias = {\n",
    "        \"60m\":\"60 Meters\",\"100m\":\"100 Meters\",\"200m\":\"200 Meters\",\"400m\":\"400 Meters\",\n",
    "        \"800m\":\"800 Meters\",\"1500m\":\"1500 Meters\",\"mile\":\"Mile\",\n",
    "        \"3k\":\"3000 Meters\",\"5k\":\"5000 Meters\",\"10k\":\"10000 Meters\",\n",
    "        \"60h\":\"60 Hurdles\",\"110h\":\"110 Hurdles\",\"100h\":\"100 Hurdles\",\n",
    "        \"4x1\":\"4 x 100 Relay\",\"4x4\":\"4 x 400 Relay\"\n",
    "    }\n",
    "    return alias.get(el, e)\n",
    "\n",
    "def parse_time_to_seconds(s: str) -> float | None:\n",
    "    if not s: return None\n",
    "    sl = s.strip().lower()\n",
    "    if sl in {\"dnf\",\"dq\",\"fs\",\"nt\",\"ns\"}: return None\n",
    "    s = s.replace(\" \", \"\")\n",
    "    if \":\" in s:\n",
    "        parts = s.split(\":\")\n",
    "        try:\n",
    "            if len(parts)==2: m,sec = int(parts[0]), float(parts[1]); return m*60+sec\n",
    "            if len(parts)==3: h,m,sec = int(parts[0]),int(parts[1]),float(parts[2]); return h*3600+m*60+sec\n",
    "        except: return None\n",
    "    try: return float(s)\n",
    "    except: return None\n",
    "\n",
    "def parse_distance_to_meters(s: str) -> float | None:\n",
    "    if not s: return None\n",
    "    sl = s.strip().lower()\n",
    "    if sl in {\"nm\"}: return None\n",
    "    if re.match(r\"^\\d{1,3}-\\d{1,2}(\\.\\d+)?$\", sl):  # ft-in\n",
    "        ft, inch = sl.split(\"-\")\n",
    "        try:\n",
    "            total_inches = int(ft)*12 + float(inch)\n",
    "            return total_inches * 0.0254\n",
    "        except: return None\n",
    "    if sl.endswith(\"m\"): sl = sl[:-1]\n",
    "    try: return float(sl)\n",
    "    except: return None\n",
    "\n",
    "def mark_to_numeric(event: str, mark: str) -> tuple[float|None, bool]:\n",
    "    if event in RUN_EVENTS:   return parse_time_to_seconds(mark), True\n",
    "    if event in FIELD_EVENTS: return parse_distance_to_meters(mark), False\n",
    "    t = parse_time_to_seconds(mark)\n",
    "    if t is not None and (\":\" in (mark or \"\") or t < 30): return t, True\n",
    "    d = parse_distance_to_meters(mark)\n",
    "    return ((d, False) if d is not None else (None, True))\n",
    "\n",
    "def safe_slug(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9]+\", \"_\", s).strip(\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4654d9",
   "metadata": {},
   "source": [
    "# Event-Family Mapping\n",
    "### Families let Indoor/Outdoor analogs share the same timeline:\n",
    "- short_sprint: 60m(Indoor) <-> 100m(Outdoor)\n",
    "- short_hurdles: 60H(Indoor) <-> 110H men / 100H women (Outdoor)\n",
    "- \"same name\" events (200, 400, TJ, etc.) map to themselves as families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d61ee215",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAME_NAME_FAMILIES = {\n",
    "    \"200 Meters\",\"400 Meters\",\"800 Meters\",\"3000 Meters\",\"5000 Meters\",\"10000 Meters\",\n",
    "    \"400 Hurdles\",\"3000 Steeplechase\",\"4 x 400 Relay\",\n",
    "    \"Long Jump\",\"Triple Jump\",\"High Jump\",\"Pole Vault\",\n",
    "    \"Shot Put\",\"Discus\",\"Hammer\",\"Javelin\",\"Weight Throw\"\n",
    "}\n",
    "\n",
    "def event_to_family(event: str, gender: str) -> str:\n",
    "    e = canonical_event(event)\n",
    "    if e in SAME_NAME_FAMILIES or e in {\"DMR\"}:\n",
    "        return e  # family == event\n",
    "    # special pairings\n",
    "    if e in {\"60 Meters\",\"100 Meters\"}:\n",
    "        return \"short_sprint\"\n",
    "    if e == \"Mile\" or e == \"1500 Meters\":\n",
    "        return \"metric_mile\"\n",
    "    if e == \"60 Hurdles\":\n",
    "        return \"short_hurdles\"\n",
    "    if e == \"110 Hurdles\" and gender.strip().lower().startswith(\"men\"):\n",
    "        return \"short_hurdles\"\n",
    "    if e == \"100 Hurdles\" and gender.strip().lower().startswith(\"women\"):\n",
    "        return \"short_hurdles\"\n",
    "    # else fallback\n",
    "    return e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095390b0",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e90ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(npy_path: str) -> pd.DataFrame:\n",
    "    arr = np.load(npy_path, allow_pickle=False)\n",
    "    df = pd.DataFrame.from_records(arr)\n",
    "    keep = [\"Division\",\"School\",\"Gender\",\"SeasonLabel\",\"SeasonYear\",\"Event\",\n",
    "            \"Athlete\",\"MarkOrTime\",\"Wind\",\"Meet\",\"MeetDate\"]\n",
    "    df = df[keep].copy()\n",
    "    for c in keep: df[c] = df[c].astype(\"string\").str.strip()\n",
    "    df[\"Event\"] = df[\"Event\"].map(canonical_event)\n",
    "    df[\"date\"]  = pd.to_datetime(df[\"MeetDate\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"date\",\"Event\",\"Athlete\",\"SeasonLabel\",\"SeasonYear\"])\n",
    "\n",
    "    # numeric mark and orientation\n",
    "    val_is_time = df.apply(lambda r: mark_to_numeric(r[\"Event\"], r[\"MarkOrTime\"]), axis=1, result_type=\"expand\")\n",
    "    df[\"value\"]   = val_is_time[0]\n",
    "    df[\"is_time\"] = val_is_time[1]\n",
    "    df = df.dropna(subset=[\"value\"])\n",
    "\n",
    "    # event-family column\n",
    "    df[\"family\"] = df.apply(lambda r: event_to_family(r[\"Event\"], r[\"Gender\"]), axis=1)\n",
    "\n",
    "    # keys\n",
    "    df[\"ath_key\"] = (df[\"Athlete\"].str.lower().str.strip() + \"||\" + df[\"Gender\"].str.strip())\n",
    "    df[\"fam_key\"] = (df[\"ath_key\"] + \"||\" + df[\"family\"])\n",
    "    df[\"season_key\"] = df[\"SeasonYear\"].astype(\"Int64\").astype(\"string\") + \"-\" + df[\"SeasonLabel\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "def agg_peak(g: pd.DataFrame) -> float:\n",
    "    # family-level season peak: min for times, max for distances\n",
    "    return g[\"value\"].min() if g[\"is_time\"].iloc[0] else g[\"value\"].max()\n",
    "\n",
    "def build_family_season_peaks(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # one row per (fam_key, season)\n",
    "    peaks = (df.groupby([\"fam_key\",\"season_key\"], as_index=False)\n",
    "               .apply(agg_peak).rename(columns={None:\"peak_value\"}))\n",
    "    meta = (df.groupby([\"fam_key\",\"season_key\"], as_index=False)\n",
    "              .agg(SeasonYear=(\"SeasonYear\",\"first\"),\n",
    "                   SeasonLabel=(\"SeasonLabel\",\"first\"),\n",
    "                   Gender=(\"Gender\",\"first\"),\n",
    "                   family=(\"family\",\"first\"),\n",
    "                   is_time=(\"is_time\",\"first\")))\n",
    "    peaks = peaks.merge(meta, on=[\"fam_key\",\"season_key\"], how=\"left\")\n",
    "    return peaks\n",
    "\n",
    "def build_mark_sequences_by_family(df: pd.DataFrame, seqlen=SEQLEN_MARKS):\n",
    "    \"\"\"\n",
    "    Precompute, for each (fam_key, season_key), a mark-level sequence X[L,F] and length L.\n",
    "    Use ONLY rows from that family in that season (e.g., short_sprint == 60m indoor, 100m outdoor).\n",
    "    \"\"\"\n",
    "    df = df.sort_values([\"fam_key\",\"season_key\",\"date\"]).reset_index(drop=True)\n",
    "    seqs = {}\n",
    "    lens = {}\n",
    "    # also record the ACTUAL EVENT used for this family+season (e.g., \"60 Meters\" vs \"100 Meters\")\n",
    "    evt_used = {}\n",
    "    for (fk, sk), g in df.groupby([\"fam_key\",\"season_key\"]):\n",
    "        vals = g[\"value\"].to_numpy()\n",
    "        days = g[\"date\"].diff().dt.days.fillna(0).clip(lower=0).to_numpy()\n",
    "        wind = pd.to_numeric(g[\"Wind\"].str.replace(\"+\",\"\", regex=False).str.replace(\"m/s\",\"\", regex=False),\n",
    "                             errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "        X = np.stack([vals, days, wind], axis=1).astype(\"float32\")\n",
    "        l = len(X)\n",
    "        if l > seqlen:\n",
    "            X = X[-seqlen:]\n",
    "            l = seqlen\n",
    "        else:\n",
    "            pad = np.zeros((seqlen-l, X.shape[1]), dtype=\"float32\")\n",
    "            X = np.vstack([pad, X])\n",
    "        seqs[(fk,sk)] = X\n",
    "        lens[(fk,sk)] = l\n",
    "        # pick the most frequent event string used in this family/season block (should be unique)\n",
    "        evt_used[(fk,sk)] = g[\"Event\"].value_counts().idxmax()\n",
    "    return seqs, lens, evt_used\n",
    "\n",
    "def build_windows_gender_family(df: pd.DataFrame, peaks: pd.DataFrame,\n",
    "                                seqs: dict, lens: dict, evt_used: dict,\n",
    "                                k_seasons=K_SEASONS):\n",
    "    \"\"\"\n",
    "    Build sliding windows of up to K past seasons to predict NEXT season with the SAME label as the last season in window.\n",
    "    Grouped by fam_key (athlete+gender+event_family). Returns:\n",
    "      X[N,K,L,F], L[N,K], label_ids[N,K], y_raw[N], evt_name[N], evt_id placeholder later, years_t[N]\n",
    "    \"\"\"\n",
    "    # sort seasons within each fam_key by (Year, label order)\n",
    "    peaks = peaks.sort_values([\"fam_key\",\"SeasonYear\",\"SeasonLabel\"], key=lambda s: s.map(SEASONLABEL_ORDER) if s.name==\"SeasonLabel\" else s)\n",
    "    # collect season list per fam_key\n",
    "    seasons_by_fk = {}\n",
    "    label_by_fk_sk = {}\n",
    "    year_by_fk_sk = {}\n",
    "    for (fk), g in peaks.groupby([\"fam_key\"]):\n",
    "        g = g.sort_values([\"SeasonYear\",\"SeasonLabel\"], key=lambda s: s.map(SEASONLABEL_ORDER) if s.name==\"SeasonLabel\" else s)\n",
    "        seasons = list(g[\"season_key\"])\n",
    "        seasons_by_fk[fk] = seasons\n",
    "        for _, r in g.iterrows():\n",
    "            label_by_fk_sk[(fk, r[\"season_key\"])] = r[\"SeasonLabel\"]\n",
    "            year_by_fk_sk[(fk, r[\"season_key\"])]  = int(r[\"SeasonYear\"])\n",
    "\n",
    "    # quick lookup for y_next (same label next year)\n",
    "    peak_map = {(r[\"fam_key\"], r[\"season_key\"]): float(r[\"peak_value\"]) for _, r in peaks.iterrows()}\n",
    "\n",
    "    X_list, L_list, lab_list, y_list, evt_list, years_t = [], [], [], [], [], []\n",
    "\n",
    "    for fk, seasons in seasons_by_fk.items():\n",
    "        if len(seasons) == 0: continue\n",
    "        # iterate windows which have a NEXT season of same label\n",
    "        for i in range(len(seasons)):\n",
    "            sk_t = seasons[i]\n",
    "            lab_t = label_by_fk_sk[(fk, sk_t)]\n",
    "            yr_t  = year_by_fk_sk[(fk, sk_t)]\n",
    "            # next same label (t+1, same SeasonLabel)\n",
    "            next_key = f\"{yr_t+1}-{lab_t}\"\n",
    "            if (fk, next_key) not in peak_map:\n",
    "                continue  # cannot make a labeled sample\n",
    "\n",
    "            # window of up to K seasons ending at i\n",
    "            start = max(0, i-(k_seasons-1))\n",
    "            window = seasons[start:i+1]\n",
    "            # left pad to K\n",
    "            pad_n = k_seasons - len(window)\n",
    "            window = [\"__PAD__\"]*pad_n + window\n",
    "\n",
    "            # assemble blocks\n",
    "            blocks, lengths, label_ids = [], [], []\n",
    "            for sk in window:\n",
    "                if sk == \"__PAD__\":\n",
    "                    blocks.append(np.zeros((SEQLEN_MARKS, 3), dtype=\"float32\"))\n",
    "                    lengths.append(0)\n",
    "                    label_ids.append(0)  # dummy; will be masked by length==0\n",
    "                else:\n",
    "                    blocks.append(seqs.get((fk, sk), np.zeros((SEQLEN_MARKS,3), dtype=\"float32\")))\n",
    "                    lengths.append(lens.get((fk, sk), 0))\n",
    "                    label_ids.append(SEASONLABEL_ID.get(label_by_fk_sk[(fk, sk)], 0))\n",
    "            Xk = np.stack(blocks, axis=0)              # [K,L,F]\n",
    "            Lk = np.array(lengths, dtype=np.int64)     # [K]\n",
    "            Labk = np.array(label_ids, dtype=np.int64) # [K]\n",
    "\n",
    "            # label in native units\n",
    "            y_next = peak_map[(fk, next_key)]\n",
    "            # event head to use = the ACTUAL EVENT name for the LAST season in window (sk_t)\n",
    "            ev_here = evt_used.get((fk, sk_t), None)\n",
    "            if ev_here is None:  # should not happen; but skip just in case\n",
    "                continue\n",
    "\n",
    "            X_list.append(Xk)\n",
    "            L_list.append(Lk)\n",
    "            lab_list.append(Labk)\n",
    "            y_list.append(y_next)\n",
    "            evt_list.append(ev_here)\n",
    "            years_t.append(yr_t)\n",
    "\n",
    "    if not X_list:\n",
    "        return None\n",
    "    X = np.stack(X_list)                      # [N,K,L,F]\n",
    "    L = np.stack(L_list)                      # [N,K]\n",
    "    LAB = np.stack(lab_list)                  # [N,K]\n",
    "    y = np.array(y_list, dtype=\"float32\")     # [N]\n",
    "    events = np.array(evt_list)\n",
    "    years_t = np.array(years_t, dtype=np.int32)\n",
    "    return X, L, LAB, y, events, years_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6853c171",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec41c04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierDS(Dataset):\n",
    "    def __init__(self, X, L, LAB, evt_id, y_z):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)       # [N,K,L,F]\n",
    "        self.L = torch.tensor(L, dtype=torch.long)          # [N,K]\n",
    "        self.LAB = torch.tensor(LAB, dtype=torch.long)      # [N,K]\n",
    "        self.e = torch.tensor(evt_id, dtype=torch.long)     # [N]\n",
    "        self.y = torch.tensor(y_z, dtype=torch.float32)     # [N]\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.L[i], self.LAB[i], self.e[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6c831b",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00577e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierGenderFamilies(nn.Module):\n",
    "    def __init__(self, in_dim, hid_season, hid_across, n_events, emb_label_dim=4):\n",
    "        super().__init__()\n",
    "        self.season_lstm = nn.LSTM(in_dim, hid_season, batch_first=True)\n",
    "        self.label_emb = nn.Embedding(num_embeddings=2, embedding_dim=emb_label_dim)  # 0=Indoor, 1=Outdoor\n",
    "        self.across_lstm = nn.LSTM(hid_season + emb_label_dim, hid_across, batch_first=True)\n",
    "        self.heads = nn.ModuleList([nn.Sequential(nn.LayerNorm(hid_across), nn.Linear(hid_across, 1))\n",
    "                                    for _ in range(n_events)])\n",
    "\n",
    "    def forward(self, x, l_season, lab_ids, evt_id):\n",
    "        \"\"\"\n",
    "        x: [B,K,L,F]\n",
    "        l_season: [B,K] (lengths per season)\n",
    "        lab_ids: [B,K] (0=Indoor,1=Outdoor) -- for non-PAD seasons\n",
    "        evt_id: [B] (which event head to use; defined by last season's event)\n",
    "        \"\"\"\n",
    "        B, K, L, F = x.shape\n",
    "        # ---- season encoder ----\n",
    "        x_flat = x.view(B*K, L, F)                   # [B*K,L,F]\n",
    "        l_flat = l_season.view(B*K)                  # [B*K]\n",
    "        l_flat = torch.clamp(l_flat, min=1)          # pack() disallows zero; PAD will be all-zeros anyway\n",
    "        packed = pack_padded_sequence(x_flat, lengths=l_flat.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, _) = self.season_lstm(packed)       # h_n: [1, B*K, hid_season]\n",
    "        season_emb = h_n[-1].view(B, K, -1)          # [B,K,hid_season]\n",
    "\n",
    "        # ---- add SeasonLabel embedding to each season ----\n",
    "        lab_emb = self.label_emb(lab_ids)            # [B,K,emb_label]\n",
    "        season_plus = torch.cat([season_emb, lab_emb], dim=-1)  # [B,K,hid_season+emb_label]\n",
    "\n",
    "        # ---- across-season encoder ----\n",
    "        # effective K lengths = number of non-empty seasons\n",
    "        k_len = (l_season > 0).sum(dim=1)            # [B]\n",
    "        k_len = torch.clamp(k_len, min=1)\n",
    "        packed_k = pack_padded_sequence(season_plus, lengths=k_len.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h2, _) = self.across_lstm(packed_k)      # h2: [1,B,hid_across]\n",
    "        fam_emb = h2[-1]                              # [B,hid_across]\n",
    "\n",
    "        # ---- event-specific heads ----\n",
    "        preds = torch.empty(B, device=x.device, dtype=torch.float32)\n",
    "        for eid in torch.unique(evt_id):\n",
    "            mask = (evt_id == eid)\n",
    "            preds[mask] = self.heads[int(eid)](fam_emb[mask]).squeeze(-1)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799dff20",
   "metadata": {},
   "source": [
    "# PreCompute and Cahce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "812c4382",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleNamespace\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Prefer fastparquet if available; never import pyarrow to avoid extension clashes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\__init__.py:62\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[32m     64\u001b[39m     ArrowDtype,\n\u001b[32m     65\u001b[39m     Int8Dtype,\n\u001b[32m     66\u001b[39m     Int16Dtype,\n\u001b[32m     67\u001b[39m     Int32Dtype,\n\u001b[32m     68\u001b[39m     Int64Dtype,\n\u001b[32m     69\u001b[39m     UInt8Dtype,\n\u001b[32m     70\u001b[39m     UInt16Dtype,\n\u001b[32m     71\u001b[39m     UInt32Dtype,\n\u001b[32m     72\u001b[39m     UInt64Dtype,\n\u001b[32m     73\u001b[39m     Float32Dtype,\n\u001b[32m     74\u001b[39m     Float64Dtype,\n\u001b[32m     75\u001b[39m     CategoricalDtype,\n\u001b[32m     76\u001b[39m     PeriodDtype,\n\u001b[32m     77\u001b[39m     IntervalDtype,\n\u001b[32m     78\u001b[39m     DatetimeTZDtype,\n\u001b[32m     79\u001b[39m     StringDtype,\n\u001b[32m     80\u001b[39m     BooleanDtype,\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[32m     82\u001b[39m     NA,\n\u001b[32m     83\u001b[39m     isna,\n\u001b[32m     84\u001b[39m     isnull,\n\u001b[32m     85\u001b[39m     notna,\n\u001b[32m     86\u001b[39m     notnull,\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[32m     88\u001b[39m     Index,\n\u001b[32m     89\u001b[39m     CategoricalIndex,\n\u001b[32m     90\u001b[39m     RangeIndex,\n\u001b[32m     91\u001b[39m     MultiIndex,\n\u001b[32m     92\u001b[39m     IntervalIndex,\n\u001b[32m     93\u001b[39m     TimedeltaIndex,\n\u001b[32m     94\u001b[39m     DatetimeIndex,\n\u001b[32m     95\u001b[39m     PeriodIndex,\n\u001b[32m     96\u001b[39m     IndexSlice,\n\u001b[32m     97\u001b[39m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[32m     98\u001b[39m     NaT,\n\u001b[32m     99\u001b[39m     Period,\n\u001b[32m    100\u001b[39m     period_range,\n\u001b[32m    101\u001b[39m     Timedelta,\n\u001b[32m    102\u001b[39m     timedelta_range,\n\u001b[32m    103\u001b[39m     Timestamp,\n\u001b[32m    104\u001b[39m     date_range,\n\u001b[32m    105\u001b[39m     bdate_range,\n\u001b[32m    106\u001b[39m     Interval,\n\u001b[32m    107\u001b[39m     interval_range,\n\u001b[32m    108\u001b[39m     DateOffset,\n\u001b[32m    109\u001b[39m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[32m    110\u001b[39m     to_numeric,\n\u001b[32m    111\u001b[39m     to_datetime,\n\u001b[32m    112\u001b[39m     to_timedelta,\n\u001b[32m    113\u001b[39m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[32m    114\u001b[39m     Flags,\n\u001b[32m    115\u001b[39m     Grouper,\n\u001b[32m    116\u001b[39m     factorize,\n\u001b[32m    117\u001b[39m     unique,\n\u001b[32m    118\u001b[39m     value_counts,\n\u001b[32m    119\u001b[39m     NamedAgg,\n\u001b[32m    120\u001b[39m     array,\n\u001b[32m    121\u001b[39m     Categorical,\n\u001b[32m    122\u001b[39m     set_eng_float_format,\n\u001b[32m    123\u001b[39m     Series,\n\u001b[32m    124\u001b[39m     DataFrame,\n\u001b[32m    125\u001b[39m )\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtseries\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\core\\api.py:47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstruction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mflags\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flags\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     48\u001b[39m     Grouper,\n\u001b[32m     49\u001b[39m     NamedAgg,\n\u001b[32m     50\u001b[39m )\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     52\u001b[39m     CategoricalIndex,\n\u001b[32m     53\u001b[39m     DatetimeIndex,\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     TimedeltaIndex,\n\u001b[32m     60\u001b[39m )\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatetimes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     62\u001b[39m     bdate_range,\n\u001b[32m     63\u001b[39m     date_range,\n\u001b[32m     64\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      2\u001b[39m     DataFrameGroupBy,\n\u001b[32m      3\u001b[39m     NamedAgg,\n\u001b[32m      4\u001b[39m     SeriesGroupBy,\n\u001b[32m      5\u001b[39m )\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GroupBy\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgrouper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Grouper\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:68\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     61\u001b[39m     GroupByApply,\n\u001b[32m     62\u001b[39m     maybe_mangle_lambdas,\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m     warn_alias_replacement,\n\u001b[32m     66\u001b[39m )\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcom\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mframe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     70\u001b[39m     base,\n\u001b[32m     71\u001b[39m     ops,\n\u001b[32m     72\u001b[39m )\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgroupby\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     74\u001b[39m     GroupBy,\n\u001b[32m     75\u001b[39m     GroupByPlot,\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m     _transform_template,\n\u001b[32m     80\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:149\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseFrameAccessor\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstruction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    145\u001b[39m     ensure_wrapped_if_datetimelike,\n\u001b[32m    146\u001b[39m     sanitize_array,\n\u001b[32m    147\u001b[39m     sanitize_masked_array,\n\u001b[32m    148\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeneric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    150\u001b[39m     NDFrame,\n\u001b[32m    151\u001b[39m     make_doc,\n\u001b[32m    152\u001b[39m )\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_key_length\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    155\u001b[39m     DatetimeIndex,\n\u001b[32m    156\u001b[39m     Index,\n\u001b[32m   (...)\u001b[39m\u001b[32m    160\u001b[39m     ensure_index_from_sequences,\n\u001b[32m    161\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:184\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    176\u001b[39m     ArrayManager,\n\u001b[32m    177\u001b[39m     BlockManager,\n\u001b[32m    178\u001b[39m     SingleArrayManager,\n\u001b[32m    179\u001b[39m )\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstruction\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    181\u001b[39m     mgr_to_mgr,\n\u001b[32m    182\u001b[39m     ndarray_to_mgr,\n\u001b[32m    183\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmethods\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdescribe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m describe_ndframe\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    186\u001b[39m     clean_fill_method,\n\u001b[32m    187\u001b[39m     clean_reindex_fill_method,\n\u001b[32m    188\u001b[39m     find_valid_index,\n\u001b[32m    189\u001b[39m )\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconcat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m concat\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\core\\methods\\describe.py:41\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrays\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfloating\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Float64Dtype\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconcat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m concat\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m format_percentiles\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcollections\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mabc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     45\u001b[39m         Hashable,\n\u001b[32m     46\u001b[39m         Sequence,\n\u001b[32m     47\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\io\\formats\\format.py:82\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtimedeltas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TimedeltaIndex\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconcat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m concat\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     83\u001b[39m     check_parent_directory,\n\u001b[32m     84\u001b[39m     stringify_path,\n\u001b[32m     85\u001b[39m )\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m printing\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:30\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtarfile\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     32\u001b[39m     IO,\n\u001b[32m     33\u001b[39m     TYPE_CHECKING,\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m     overload,\n\u001b[32m     42\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     44\u001b[39m     urljoin,\n\u001b[32m     45\u001b[39m     urlparse \u001b[38;5;28;01mas\u001b[39;00m parse_url,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     uses_relative,\n\u001b[32m     49\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\tarfile.py:51\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpwd\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     53\u001b[39m     pwd = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1322\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1262\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1559\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1533\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1632\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:152\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os, json, random\n",
    "from datetime import datetime\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Prefer fastparquet if available; never import pyarrow to avoid extension clashes\n",
    "try:\n",
    "    import fastparquet  # noqa: F401\n",
    "    _HAS_FASTPARQUET = True\n",
    "except Exception:\n",
    "    _HAS_FASTPARQUET = False\n",
    "\n",
    "CACHE_SUBDIR = \"cache\"\n",
    "\n",
    "def _pkl_path(parquet_path: str) -> str:\n",
    "    root, _ = os.path.splitext(parquet_path)\n",
    "    return root + \".pkl\"\n",
    "\n",
    "def deflate_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Normalize to plain dtypes (no Pandas/Arrow extension types).\"\"\"\n",
    "    out = df.copy()\n",
    "    import pandas.api.types as ptypes\n",
    "    for c in out.columns:\n",
    "        s = out[c]\n",
    "        # handle pandas Period explicitly\n",
    "        if getattr(s.dtype, \"kind\", None) is None and str(s.dtype).startswith(\"period[\"):\n",
    "            out[c] = s.astype(\"string\").astype(str)\n",
    "        elif ptypes.is_datetime64_any_dtype(s):\n",
    "            out[c] = pd.to_datetime(s).astype(\"datetime64[ns]\")\n",
    "        elif ptypes.is_integer_dtype(s):\n",
    "            out[c] = s.astype(\"int64\")\n",
    "        elif ptypes.is_float_dtype(s):\n",
    "            out[c] = s.astype(\"float64\")\n",
    "        elif ptypes.is_bool_dtype(s):\n",
    "            out[c] = s.astype(bool)\n",
    "        else:\n",
    "            out[c] = s.astype(str)\n",
    "    return out\n",
    "\n",
    "def save_table(df: pd.DataFrame, parquet_path: str):\n",
    "    df2 = deflate_df(df)\n",
    "    if _HAS_FASTPARQUET:\n",
    "        df2.to_parquet(parquet_path, index=False, engine=\"fastparquet\")\n",
    "    else:\n",
    "        df2.to_pickle(_pkl_path(parquet_path))\n",
    "\n",
    "def load_table(parquet_path: str) -> pd.DataFrame:\n",
    "    pkl = _pkl_path(parquet_path)\n",
    "    if _HAS_FASTPARQUET and os.path.exists(parquet_path):\n",
    "        return pd.read_parquet(parquet_path, engine=\"fastparquet\")\n",
    "    if os.path.exists(pkl):\n",
    "        return pd.read_pickle(pkl)\n",
    "    raise FileNotFoundError(f\"Not found: {parquet_path} or {pkl}\")\n",
    "\n",
    "def save_dict_table(d: dict, parquet_path: str, value_name: str):\n",
    "    ser = pd.Series(d)\n",
    "    ser.index = pd.MultiIndex.from_tuples(ser.index, names=[\"fam_key\",\"season_key\"])\n",
    "    df = ser.to_frame(name=value_name).reset_index()\n",
    "    save_table(df, parquet_path)\n",
    "\n",
    "def load_dict_table(parquet_path: str) -> dict:\n",
    "    df = load_table(parquet_path)\n",
    "    val_col = df.columns[-1]\n",
    "    return {(r.fam_key, r.season_key): r[val_col] for _, r in df.iterrows()}\n",
    "\n",
    "def save_seqs_npz(seqs: dict, path: str):\n",
    "    np.savez_compressed(path, **{f\"{fk}§{sk}\": arr for (fk, sk), arr in seqs.items()})\n",
    "\n",
    "def load_seqs_npz(path: str) -> dict:\n",
    "    data = np.load(path, allow_pickle=False)\n",
    "    return {tuple(k.split(\"§\", 1)): data[k] for k in data.files}\n",
    "\n",
    "# ---------------- your functions using the helpers ----------------\n",
    "def cache_paths(outdir: str) -> dict:\n",
    "    cdir = os.path.join(outdir, CACHE_SUBDIR)\n",
    "    os.makedirs(cdir, exist_ok=True)\n",
    "    return {\n",
    "        \"dir\": cdir,\n",
    "        \"df\": os.path.join(cdir, \"marks_prepared.parquet\"),  # may write .pkl\n",
    "        \"peaks\": os.path.join(cdir, \"peaks_all.parquet\"),\n",
    "        \"lens\": os.path.join(cdir, \"lens.parquet\"),\n",
    "        \"evt\": os.path.join(cdir, \"evt_used.parquet\"),\n",
    "        \"seqs\": os.path.join(cdir, \"seqs.npz\"),\n",
    "        \"manifest\": os.path.join(cdir, \"manifest.json\"),\n",
    "    }\n",
    "\n",
    "def save_dict_to_parquet(d: dict, path: str, value_name: str):\n",
    "    # kept for compatibility\n",
    "    save_dict_table(d, path, value_name)\n",
    "\n",
    "def load_dict_from_parquet(path: str) -> dict:\n",
    "    return load_dict_table(path)\n",
    "\n",
    "def set_seed_all(seed: int):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# ------------------ configs -----------------------\n",
    "def build_defaults() -> SimpleNamespace:\n",
    "    return SimpleNamespace(\n",
    "        input=NPY_PATH_DEFAULT,\n",
    "        outdir=OUT_DIR_DEFAULT,\n",
    "        seqlen=SEQLEN_MARKS,\n",
    "        kseasons=K_SEASONS,\n",
    "        hid_season=HID_SEASON,\n",
    "        hid_across=HID_ACROSS,\n",
    "        emb_label=EMB_LABEL,\n",
    "        epochs=EPOCHS,\n",
    "        min_train_per_event=MIN_TRAIN_PER_EVENT,\n",
    "    )\n",
    "\n",
    "# ============================================================\n",
    "# %% BLOCK 1 — PRECOMPUTE & CACHE (run this once)\n",
    "# ============================================================\n",
    "\n",
    "args = build_defaults()\n",
    "set_seed_all(SEED)\n",
    "\n",
    "os.makedirs(args.outdir, exist_ok=True)\n",
    "paths = cache_paths(args.outdir)\n",
    "\n",
    "print(\"🧮 Loading and preparing marks…\")\n",
    "df = load_df(args.input)  # your heavy step (parses times/distances, dates, etc.)\n",
    "print(f\"   rows in df: {len(df):,}\")\n",
    "save_table(df, paths[\"df\"])\n",
    "\n",
    "print(\"🏔️  Building family-season peaks…\")\n",
    "peaks_all = build_family_season_peaks(df)\n",
    "print(f\"   rows in peaks_all: {len(peaks_all):,}\")\n",
    "save_table(peaks_all, paths[\"peaks\"])\n",
    "\n",
    "print(f\"📦 Building per-season sequences (seqlen={args.seqlen}) …\")\n",
    "seqs, lens, evt_used = build_mark_sequences_by_family(df, seqlen=args.seqlen)\n",
    "print(f\"   seasons with sequences: {len(seqs):,}\")\n",
    "\n",
    "# ---- save all artifacts ----\n",
    "print(f\"💾 Saving cache → {paths['dir']}\")\n",
    "save_table(df, paths[\"df\"])\n",
    "save_table(peaks_all, paths[\"peaks\"])\n",
    "save_dict_table(lens, paths[\"lens\"], \"orig_len\")\n",
    "save_dict_table(evt_used, paths[\"evt\"], \"event\")\n",
    "save_seqs_npz(seqs, paths[\"seqs\"])\n",
    "with open(paths[\"manifest\"], \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"created_utc\": datetime.utcnow().isoformat()+\"Z\"}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911be1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args: SimpleNamespace | None = None):  # <-- set to True to run this block\n",
    "    args = build_defaults()\n",
    "    set_seed_all()\n",
    "\n",
    "    os.makedirs(args.outdir, exist_ok=True)\n",
    "    paths = cache_paths(args.outdir)\n",
    "\n",
    "    # ---- load cached artifacts (no recompute!) ----\n",
    "    print(\"📦 Loading cache…\")\n",
    "    if not all(os.path.exists(p) for p in [paths[\"df\"], paths[\"peaks\"], paths[\"lens\"], paths[\"evt\"], paths[\"seqs\"]]):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Cache missing. Run the PRECOMPUTE block first.\\n\"\n",
    "            f\"Expected: {paths['df']}, {paths['peaks']}, {paths['lens']}, {paths['evt']}, {paths['seqs']}\"\n",
    "        )\n",
    "    df         = pd.read_parquet(paths[\"df\"])\n",
    "    peaks_all  = pd.read_parquet(paths[\"peaks\"])\n",
    "    lens       = load_dict_from_parquet(paths[\"lens\"])\n",
    "    evt_used   = load_dict_from_parquet(paths[\"evt\"])\n",
    "    seqs       = load_seqs_npz(paths[\"seqs\"])\n",
    "\n",
    "    # ---- training loop (unchanged, but using cached data) ----\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    genders = sorted(df[\"Gender\"].dropna().unique())\n",
    "    summary_rows = []\n",
    "\n",
    "    for gender in genders:\n",
    "        F_marks = df[df[\"Gender\"]==gender].copy()\n",
    "        F_peaks = peaks_all[peaks_all[\"Gender\"]==gender].copy()\n",
    "        if F_marks.empty or F_peaks.empty:\n",
    "            continue\n",
    "\n",
    "        built = build_windows_gender_family(F_marks, F_peaks, seqs, lens, evt_used,\n",
    "                                            k_seasons=args.kseasons)\n",
    "        if built is None:\n",
    "            continue\n",
    "        X, Lk, LAB, y_raw, events, years_t = built   # [N,K,L,F], [N,K], [N,K], [N], [N], [N]\n",
    "\n",
    "        # ====== TIME-BASED SPLIT (no test) ======\n",
    "        tr_m = years_t <= 2024\n",
    "        va_m = years_t == 2025\n",
    "\n",
    "        if not tr_m.any():\n",
    "            idx = np.arange(len(X)); np.random.shuffle(idx)\n",
    "            n=len(idx); a=int(0.8*n)\n",
    "            tr_m = np.zeros(n,bool); va_m=np.zeros(n,bool)\n",
    "            tr_m[idx[:a]]=True; va_m[idx[a:]]=True\n",
    "\n",
    "        # Filter events by TRAIN count (per-event head)\n",
    "        tr_events, tr_counts = np.unique(events[tr_m], return_counts=True)\n",
    "        allowed = set(e for e,c in zip(tr_events, tr_counts) if c >= args.min_train_per_event)\n",
    "        keep = np.array([e in allowed for e in events])\n",
    "        X, Lk, LAB, y_raw, events, years_t = X[keep], Lk[keep], LAB[keep], y_raw[keep], events[keep], years_t[keep]\n",
    "        tr_m, va_m = tr_m[keep], va_m[keep]\n",
    "\n",
    "        if len(allowed) == 0:\n",
    "            print(f\"Skipping gender {gender}: no event with ≥{args.min_train_per_event} train samples\")\n",
    "            continue\n",
    "\n",
    "        # event ids (per-event heads & per-event standardization)\n",
    "        evt_list = sorted(list(allowed))\n",
    "        evt2id = {e:i for i,e in enumerate(evt_list)}\n",
    "        evt_id = np.array([evt2id[e] for e in events], dtype=np.int64)\n",
    "\n",
    "        # per-event z-score: fit on TRAIN only\n",
    "        y_z = y_raw.copy()\n",
    "        mu_by_evt, sd_by_evt = {}, {}\n",
    "        for e in evt_list:\n",
    "            idx_tr = (evt_id==evt2id[e]) & tr_m\n",
    "            mu = y_raw[idx_tr].mean() if idx_tr.any() else 0.0\n",
    "            sd = y_raw[idx_tr].std()  if idx_tr.any() else 1.0\n",
    "            if sd < 1e-6: sd = 1.0\n",
    "            mu_by_evt[e], sd_by_evt[e] = float(mu), float(sd)\n",
    "            idx_all = (evt_id==evt2id[e])\n",
    "            y_z[idx_all] = (y_raw[idx_all] - mu) / sd\n",
    "\n",
    "        # DataLoaders (train + val only)\n",
    "        def pack(mask):\n",
    "            return X[mask], Lk[mask], LAB[mask], evt_id[mask], y_z[mask], y_raw[mask]\n",
    "        Xtr,Ltr,LABtr,Etr,ytr_z,ytr = pack(tr_m)\n",
    "        Xva,Lva,LABva,Eva,yva_z,yva = pack(va_m)\n",
    "\n",
    "        dl_tr = DataLoader(HierDS(Xtr,Ltr,LABtr,Etr,ytr_z), batch_size=BATCH, shuffle=True)\n",
    "        dl_va = DataLoader(HierDS(Xva,Lva,LABva,Eva,yva_z), batch_size=256)\n",
    "\n",
    "        # Model\n",
    "        model = HierGenderFamilies(in_dim=X.shape[-1], hid_season=args.hid_season,\n",
    "                                   hid_across=args.hid_across, n_events=len(evt_list),\n",
    "                                   emb_label_dim=args.emb_label).to(device)\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "        loss_fn = nn.L1Loss()\n",
    "\n",
    "        def run_epoch(dl, train=True):\n",
    "            model.train(train); tot=0; n=0\n",
    "            for xb, lb, labb, eb, yb in dl:\n",
    "                xb, lb, labb, eb, yb = xb.to(device), lb.to(device), labb.to(device), eb.to(device), yb.to(device)\n",
    "                with torch.set_grad_enabled(train):\n",
    "                    pred = model(xb, lb, labb, eb)\n",
    "                    loss = loss_fn(pred, yb)\n",
    "                if train:\n",
    "                    opt.zero_grad(); loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    opt.step()\n",
    "                tot += loss.item()*len(xb); n+=len(xb)\n",
    "            return tot/max(n,1)\n",
    "\n",
    "        best=1e9; patience=0; best_state=None\n",
    "        for ep in range(EPOCHS):\n",
    "            tr = run_epoch(dl_tr, True)\n",
    "            va = run_epoch(dl_va, False)\n",
    "            print(f\"[Hier-GenderFamilies] {gender} ep {ep:02d}  train L1(z)={tr:.3f}  val L1(z)={va:.3f}\")\n",
    "            if va < best - 1e-3:\n",
    "                best, patience = va, 0\n",
    "                best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            else:\n",
    "                patience += 1\n",
    "            if patience >= 5: break\n",
    "        if best_state is not None: model.load_state_dict(best_state)\n",
    "\n",
    "        # ==== VALIDATION METRICS IN NATIVE UNITS ====\n",
    "        model.eval()\n",
    "        preds_z_val, Eva_list = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, lb, labb, eb, yb in dl_va:\n",
    "                xb, lb, labb, eb = xb.to(device), lb.to(device), labb.to(device), eb.to(device)\n",
    "                preds_z_val.append(model(xb, lb, labb, eb).cpu().numpy())\n",
    "                Eva_list.append(eb.cpu().numpy())\n",
    "        preds_z_val = np.concatenate(preds_z_val) if preds_z_val else np.array([])\n",
    "        Eva_all = np.concatenate(Eva_list) if Eva_list else np.array([], dtype=np.int64)\n",
    "        yhat_val = np.empty_like(preds_z_val)\n",
    "        for eid, e in enumerate(evt_list):\n",
    "            idx = (Eva_all == eid)\n",
    "            if idx.any():\n",
    "                mu, sd = mu_by_evt[e], sd_by_evt[e]\n",
    "                yhat_val[idx] = preds_z_val[idx]*sd + mu\n",
    "        overall_val_mae = float(np.mean(np.abs(yhat_val - yva))) if len(yhat_val) else float(\"nan\")\n",
    "\n",
    "        # per-event MAE (validation)\n",
    "        per_event_val = []\n",
    "        for eid, e in enumerate(evt_list):\n",
    "            idx = (Eva_all == eid)\n",
    "            if idx.any():\n",
    "                mae = float(np.mean(np.abs(yhat_val[idx] - yva[idx])))\n",
    "                per_event_val.append({\"event\":e, \"val_mae\":mae, \"n\":int(idx.sum())})\n",
    "\n",
    "        fam_dir = os.path.join(args.outdir, f\"{safe_slug(gender)}\")\n",
    "        os.makedirs(fam_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(fam_dir,\"model.pth\"))\n",
    "        meta = {\n",
    "            \"model_name\":\"Hier-GenderFamilies\",\n",
    "            \"gender\":gender,\n",
    "            \"events\":evt_list, \"event2id\": {e:i for i,e in enumerate(evt_list)},\n",
    "            \"mu_by_event\":mu_by_evt, \"sd_by_event\":sd_by_evt,\n",
    "            \"seqlen_marks\":args.seqlen, \"k_seasons\":args.kseasons,\n",
    "            \"hid_season\":args.hid_season, \"hid_across\":args.hid_across, \"emb_label\":args.emb_label,\n",
    "            \"in_dim\":X.shape[-1],\n",
    "            \"created\": datetime.utcnow().isoformat() + \"Z\",\n",
    "            \"val_L1_z\": float(best),\n",
    "            \"val_mae_overall_native\": overall_val_mae,\n",
    "            \"per_event_val_metrics\": per_event_val,\n",
    "            \"test_mae_overall\": None,\n",
    "            \"per_event_test_metrics\": []\n",
    "        }\n",
    "        with open(os.path.join(fam_dir,\"meta.json\"),\"w\",encoding=\"utf-8\") as f:\n",
    "            json.dump(meta,f,indent=2)\n",
    "\n",
    "        row = {\"gender\":gender,\"overall_val_mae\":overall_val_mae,\"n_events\":len(evt_list)}\n",
    "        for pe in per_event_val:\n",
    "            row[f\"ValMAE_{pe['event']}\"]=pe[\"val_mae\"]\n",
    "        summary_rows.append(row)\n",
    "\n",
    "    # save summary\n",
    "    if summary_rows:\n",
    "        pd.DataFrame(summary_rows).to_csv(os.path.join(args.outdir,\"summary_metrics.csv\"), index=False)\n",
    "        print(f\"✅ Saved models → {args.outdir}\")\n",
    "    else:\n",
    "        print(\"⚠️ No models trained (insufficient data or filters too strict).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80f5c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_318044\\2038011872.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"family\"] = df.apply(lambda r: event_to_family(r[\"Event\"], r[\"Gender\"]), axis=1)\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_318044\\2038011872.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"ath_key\"] = (df[\"Athlete\"].str.lower().str.strip() + \"||\" + df[\"Gender\"].str.strip())\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_318044\\2038011872.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"fam_key\"] = (df[\"ath_key\"] + \"||\" + df[\"family\"])\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_318044\\2038011872.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"season_key\"] = df[\"SeasonYear\"].astype(\"Int64\").astype(\"string\") + \"-\" + df[\"SeasonLabel\"]\n",
      "11037.53s - unexpected character after line continuation character (<string>, line 1)\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\debugpy\\_vendored\\pydevd\\_pydevd_bundle\\pydevd_vars.py\", line 629, in change_attr_expression\n",
      "    value = eval(expression, frame.f_globals, frame.f_locals)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 1\n",
      "    Division             School Gender SeasonLabel SeasonYear  \\0          NCAA Division I  Abilene Christian    Men     Indoors       2024   1          NCAA Division I  Abilene Christian    Men     Indoors       2024   2          NCAA Division I  Abilene Christian    Men     Indoors       2024   3          NCAA Division I  Abilene Christian    Men     Indoors       2024   4          NCAA Division I  Abilene Christian    Men     Indoors       2024   ...                    ...                ...    ...         ...        ...   3739084  NCAA Division III         York (Pa.)  Women    Outdoors       2021   3739085  NCAA Division III         York (Pa.)  Women    Outdoors       2021   3739086  NCAA Division III         York (Pa.)  Women    Outdoors       2021   3739087  NCAA Division III         York (Pa.)  Women    Outdoors       2021   3739088  NCAA Division III         York (Pa.)  Women    Outdoors       2021            ... is_time        family             ath_key  \\0        ...    True  short_sprint  cooksey, sean||Men   1        ...    True  short_sprint  krause, ethan||Men   2        ...    True  short_sprint  cooksey, sean||Men   3        ...    True  short_sprint  cooksey, sean||Men   4        ...    True  short_sprint  cooksey, sean||Men   ...      ...     ...           ...                 ...   3739084  ...   False       Javelin   mcgee, amy||Women   3739085  ...   False       Javelin   mcgee, amy||Women   3739086  ...   False       Javelin  brown, erin||Women   3739087  ...   False       Javelin  brown, erin||Women   3739088  ...   False       Javelin  brown, erin||Women                                     fam_key     season_key  0        cooksey, sean||Men||short_sprint   2024-Indoors  1        krause, ethan||Men||short_sprint   2024-Indoors  2        cooksey, sean||Men||short_sprint   2024-Indoors  3        cooksey, sean||Men||short_sprint   2024-Indoors  4        cooksey, sean||Men||short_sprint   2024-Indoors  ...                                   ...            ...  3739084        mcgee, amy||Women||Javelin  2021-Outdoors  3739085        mcgee, amy||Women||Javelin  2021-Outdoors  3739086       brown, erin||Women||Javelin  2021-Outdoors  3739087       brown, erin||Women||Javelin  2021-Outdoors  3739088       brown, erin||Women||Javelin  2021-Outdoors  [3722351 rows x 18 columns]\n",
      "                                                                ^\n",
      "SyntaxError: unexpected character after line continuation character\n",
      "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_318044\\2038011872.py:35: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(agg_peak).rename(columns={None:\"peak_value\"}))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m F_marks.empty \u001b[38;5;129;01mor\u001b[39;00m F_peaks.empty:\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m built = \u001b[43mbuild_windows_gender_family\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF_marks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_peaks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevt_used\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43mk_seasons\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkseasons\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m built \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 89\u001b[39m, in \u001b[36mbuild_windows_gender_family\u001b[39m\u001b[34m(df, peaks, seqs, lens, evt_used, k_seasons)\u001b[39m\n\u001b[32m     87\u001b[39m year_by_fk_sk = {}\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (fk), g \u001b[38;5;129;01min\u001b[39;00m peaks.groupby([\u001b[33m\"\u001b[39m\u001b[33mfam_key\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     g = \u001b[43mg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSeasonYear\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSeasonLabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSEASONLABEL_ORDER\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m==\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSeasonLabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     seasons = \u001b[38;5;28mlist\u001b[39m(g[\u001b[33m\"\u001b[39m\u001b[33mseason_key\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     91\u001b[39m     seasons_by_fk[fk] = seasons\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:7190\u001b[39m, in \u001b[36mDataFrame.sort_values\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   7183\u001b[39m         \u001b[38;5;66;03m# error: List comprehension has incompatible type List[Series];\u001b[39;00m\n\u001b[32m   7184\u001b[39m         \u001b[38;5;66;03m# expected List[ndarray]\u001b[39;00m\n\u001b[32m   7185\u001b[39m         keys = [\n\u001b[32m   7186\u001b[39m             Series(k, name=name)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   7187\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m (k, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keys, by)\n\u001b[32m   7188\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m7190\u001b[39m     indexer = \u001b[43mlexsort_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   7191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mascending\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\n\u001b[32m   7192\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7193\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(by):\n\u001b[32m   7194\u001b[39m     \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   7196\u001b[39m     k = \u001b[38;5;28mself\u001b[39m._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\core\\sorting.py:346\u001b[39m, in \u001b[36mlexsort_indexer\u001b[39m\u001b[34m(keys, orders, na_position, key, codes_given)\u001b[39m\n\u001b[32m    343\u001b[39m labels = []\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, order \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keys, orders):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     k = \u001b[43mensure_key_mapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m codes_given:\n\u001b[32m    348\u001b[39m         codes = cast(np.ndarray, k)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\core\\sorting.py:569\u001b[39m, in \u001b[36mensure_key_mapped\u001b[39m\u001b[34m(values, key, levels)\u001b[39m\n\u001b[32m    566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, ABCMultiIndex):\n\u001b[32m    567\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _ensure_key_mapped_multiindex(values, key, level=levels)\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m result = key(\u001b[43mvalues\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) != \u001b[38;5;28mlen\u001b[39m(values):\n\u001b[32m    571\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    572\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUser-provided `key` function must not change the shape of the array.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    573\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py:6830\u001b[39m, in \u001b[36mNDFrame.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m   6681\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   6682\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: bool_t | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Self:\n\u001b[32m   6683\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6684\u001b[39m \u001b[33;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[32m   6685\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   6828\u001b[39m \u001b[33;03m    dtype: int64\u001b[39;00m\n\u001b[32m   6829\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6830\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6831\u001b[39m     \u001b[38;5;28mself\u001b[39m._clear_item_cache()\n\u001b[32m   6832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(data, axes=data.axes).__finalize__(\n\u001b[32m   6833\u001b[39m         \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6834\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:593\u001b[39m, in \u001b[36mBaseBlockManager.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    591\u001b[39m         new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcopy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m res.axes = new_axes\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m    597\u001b[39m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:364\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    363\u001b[39m         applied = \u001b[38;5;28mgetattr\u001b[39m(b, f)(**kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m     result_blocks = \u001b[43mextend_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapplied\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_blocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\OneDrive\\Desktop\\FlightPhase\\.venv\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:2868\u001b[39m, in \u001b[36mextend_blocks\u001b[39m\u001b[34m(result, blocks)\u001b[39m\n\u001b[32m   2862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m values, dtype\n\u001b[32m   2865\u001b[39m \u001b[38;5;66;03m# -----------------------------------------------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2868\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextend_blocks\u001b[39m(result, blocks=\u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[32m   2869\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"return a new extended blocks, given the result\"\"\"\u001b[39;00m\n\u001b[32m   2870\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c50a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
