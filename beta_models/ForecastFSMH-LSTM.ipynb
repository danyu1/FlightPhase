{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "941a1163",
   "metadata": {},
   "source": [
    "# Pre Process and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7dbcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse, os, re, json, random, math\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ------------------ config ------------------\n",
    "SEQLEN = 32\n",
    "EPOCHS = 30\n",
    "BATCH = 128\n",
    "LR = 1e-3\n",
    "HID = 64\n",
    "MIN_TRAIN_SAMPLES = 60   # skip tiny tasks\n",
    "OUT_DIR_DEFAULT = \"models_ESS_LSTM\"\n",
    "NPY_PATH_DEFAULT = \"tfrrs_performances_fast.npy\"\n",
    "SEED = 42\n",
    "\n",
    "# ------------------ utils -------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "RUN_EVENTS = {\n",
    "    \"60 Meters\",\"100 Meters\",\"200 Meters\",\"400 Meters\",\"800 Meters\",\"1500 Meters\",\n",
    "    \"Mile\",\"3000 Meters\",\"5000 Meters\",\"10000 Meters\",\"60 Hurdles\",\"110 Hurdles\",\n",
    "    \"400 Hurdles\",\"3000 Steeplechase\",\"DMR\",\"4 x 100 Relay\",\"4 x 400 Relay\"\n",
    "}\n",
    "FIELD_EVENTS = {\n",
    "    \"Long Jump\",\"Triple Jump\",\"High Jump\",\"Pole Vault\",\n",
    "    \"Shot Put\",\"Discus\",\"Hammer\",\"Javelin\",\"Weight Throw\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb54840",
   "metadata": {},
   "source": [
    "# Define Clean Up Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafee08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonical_event(e: str) -> str:\n",
    "    if e in RUN_EVENTS or e in FIELD_EVENTS: return e\n",
    "    el = (e or \"\").strip().lower()\n",
    "    alias = {\n",
    "        \"60m\":\"60 Meters\",\"100m\":\"100 Meters\",\"200m\":\"200 Meters\",\"400m\":\"400 Meters\",\n",
    "        \"800m\":\"800 Meters\",\"1500m\":\"1500 Meters\",\"mile\":\"Mile\",\n",
    "        \"3k\":\"3000 Meters\",\"5k\":\"5000 Meters\",\"10k\":\"10000 Meters\",\n",
    "    }\n",
    "    return alias.get(el, e)\n",
    "\n",
    "def parse_time_to_seconds(s: str) -> float | None:\n",
    "    if not s: return None\n",
    "    sl = s.strip().lower()\n",
    "    if sl in {\"dnf\",\"dq\",\"fs\",\"nt\",\"ns\"}: return None\n",
    "    s = s.replace(\" \", \"\")\n",
    "    if \":\" in s:\n",
    "        parts = s.split(\":\")\n",
    "        try:\n",
    "            if len(parts)==2: m,sec = int(parts[0]), float(parts[1]); return m*60+sec\n",
    "            if len(parts)==3: h,m,sec = int(parts[0]),int(parts[1]),float(parts[2]); return h*3600+m*60+sec\n",
    "        except: return None\n",
    "    try: return float(s)\n",
    "    except: return None\n",
    "\n",
    "def parse_distance_to_meters(s: str) -> float | None:\n",
    "    if not s: return None\n",
    "    sl = s.strip().lower()\n",
    "    if sl in {\"nm\"}: return None\n",
    "    if re.match(r\"^\\d{1,3}-\\d{1,2}(\\.\\d+)?$\", sl):  # ft-in\n",
    "        ft, inch = sl.split(\"-\")\n",
    "        try:\n",
    "            total_inches = int(ft)*12 + float(inch)\n",
    "            return total_inches * 0.0254\n",
    "        except: return None\n",
    "    if sl.endswith(\"m\"): sl = sl[:-1]\n",
    "    try: return float(sl)\n",
    "    except: return None\n",
    "\n",
    "def mark_to_numeric(event: str, mark: str) -> tuple[float|None, bool]:\n",
    "    if event in RUN_EVENTS:   return parse_time_to_seconds(mark), True\n",
    "    if event in FIELD_EVENTS: return parse_distance_to_meters(mark), False\n",
    "    t = parse_time_to_seconds(mark)\n",
    "    if t is not None and (\":\" in (mark or \"\") or t < 30): return t, True\n",
    "    d = parse_distance_to_meters(mark)\n",
    "    return ((d, False) if d is not None else (None, True))\n",
    "\n",
    "def load_df(npy_path: str) -> pd.DataFrame:\n",
    "    arr = np.load(npy_path, allow_pickle=False)\n",
    "    df = pd.DataFrame.from_records(arr)\n",
    "    keep = [\"Division\",\"School\",\"Gender\",\"SeasonLabel\",\"SeasonYear\",\"Event\",\n",
    "            \"Athlete\",\"MarkOrTime\",\"Wind\",\"Meet\",\"MeetDate\"]\n",
    "    df = df[keep].copy()\n",
    "    for c in keep: df[c] = df[c].astype(\"string\").str.strip()\n",
    "    df[\"Event\"] = df[\"Event\"].map(canonical_event)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"MeetDate\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"date\",\"Event\",\"Athlete\",\"SeasonLabel\",\"SeasonYear\"])\n",
    "    val_is_time = df.apply(lambda r: mark_to_numeric(r[\"Event\"], r[\"MarkOrTime\"]), axis=1, result_type=\"expand\")\n",
    "    df[\"value\"] = val_is_time[0]; df[\"is_time\"] = val_is_time[1]\n",
    "    df = df.dropna(subset=[\"value\"])\n",
    "    df[\"key\"] = df[\"Athlete\"].str.lower().str.strip() + \"||\" + df[\"Event\"].str.lower().str.strip() + \"||\" + df[\"Gender\"]\n",
    "    df[\"season_key\"] = df[\"SeasonYear\"].astype(\"Int64\").astype(\"string\") + \"-\" + df[\"SeasonLabel\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f735c9b",
   "metadata": {},
   "source": [
    "# Next Season Peak Per Athelte + Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5017e36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_peak(g: pd.DataFrame) -> float:\n",
    "    return g[\"value\"].min() if g[\"is_time\"].iloc[0] else g[\"value\"].max()\n",
    "\n",
    "def build_targets(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    peaks = (df.groupby([\"key\",\"season_key\"], as_index=False)\n",
    "               .apply(agg_peak).rename(columns={None:\"peak_value\"}))\n",
    "    meta = (df.groupby([\"key\",\"season_key\"], as_index=False)\n",
    "              .agg(SeasonYear=(\"SeasonYear\",\"first\"),\n",
    "                   SeasonLabel=(\"SeasonLabel\",\"first\"),\n",
    "                   Event=(\"Event\",\"first\"),\n",
    "                   Gender=(\"Gender\",\"first\"),\n",
    "                   is_time=(\"is_time\",\"first\")))\n",
    "    peaks = peaks.merge(meta, on=[\"key\",\"season_key\"], how=\"left\")\n",
    "    def next_season(label, year): return f\"{int(year+1)}-{label}\"\n",
    "    peaks[\"next_season_key\"] = peaks.apply(lambda r: next_season(r[\"SeasonLabel\"], int(r[\"SeasonYear\"])), axis=1)\n",
    "    out = peaks.merge(peaks[[\"key\",\"season_key\",\"peak_value\"]]\n",
    "                      .rename(columns={\"season_key\":\"next_season_key\",\"peak_value\":\"y_next\"}),\n",
    "                      on=[\"key\",\"next_season_key\"], how=\"left\").drop(columns=[\"next_season_key\"])\n",
    "    out = out.dropna(subset=[\"y_next\"]).reset_index(drop=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa80dfb",
   "metadata": {},
   "source": [
    "# Build SequencesFrom Raw Marks Up to the End of the Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676c2ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(df: pd.DataFrame, seqlen=SEQLEN):\n",
    "    df = df.sort_values([\"key\",\"date\"]).reset_index(drop=True)\n",
    "    def make_seq(g):\n",
    "        vals = g[\"value\"].to_numpy()\n",
    "        days = g[\"date\"].diff().dt.days.fillna(0).clip(lower=0).to_numpy()\n",
    "        wind = pd.to_numeric(g[\"Wind\"].str.replace(\"+\",\"\", regex=False).str.replace(\"m/s\",\"\", regex=False),\n",
    "                             errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "        X = np.stack([vals, days, wind], axis=1).astype(\"float32\")\n",
    "        return X[-seqlen:] if len(X)>seqlen else X\n",
    "    seqs = {}\n",
    "    for (k,s), g in df.groupby([\"key\",\"season_key\"]): seqs[(k,s)] = make_seq(g)\n",
    "    return seqs\n",
    "\n",
    "def pad_seq(X, L=SEQLEN):\n",
    "    if len(X)>=L: return X\n",
    "    pad = np.zeros((L-len(X), X.shape[1]), dtype=\"float32\")\n",
    "    return np.vstack([pad, X])\n",
    "\n",
    "class FamDS(Dataset):\n",
    "    def __init__(self, X, evt_ids, y_z):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.evt = torch.tensor(evt_ids, dtype=torch.long)\n",
    "        self.y = torch.tensor(y_z, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.evt[i], self.y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b780c048",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f02dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedEncoderPerEventHeads(nn.Module):\n",
    "    def __init__(self, in_dim, hid, n_events):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, hid, batch_first=True)\n",
    "        self.heads = nn.ModuleList([nn.Sequential(nn.LayerNorm(hid), nn.Linear(hid, 1))\n",
    "                                    for _ in range(n_events)])\n",
    "    def forward(self, x, evt_id):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        preds = torch.empty(x.size(0), device=x.device, dtype=torch.float32)\n",
    "        # route each sub-batch to its event head\n",
    "        for eid in torch.unique(evt_id):\n",
    "            mask = (evt_id == eid)\n",
    "            preds[mask] = self.heads[int(eid)](last[mask]).squeeze(-1)\n",
    "        return preds\n",
    "\n",
    "def safe_slug(s: str) -> str:\n",
    "    return re.sub(r\"[^A-Za-z0-9]+\", \"_\", s).strip(\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480acc6e",
   "metadata": {},
   "source": [
    "# Training All Families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dab0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    set_seed(SEED)\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"-i\",\"--input\", default=NPY_PATH_DEFAULT)\n",
    "    ap.add_argument(\"-o\",\"--outdir\", default=OUT_DIR_DEFAULT)\n",
    "    ap.add_argument(\"--min-train-per-event\", type=int, default=MIN_TRAIN_PER_EVENT)\n",
    "    ap.add_argument(\"--epochs\", type=int, default=EPOCHS)\n",
    "    ap.add_argument(\"--hid\", type=int, default=HID)\n",
    "    ap.add_argument(\"--seqlen\", type=int, default=SEQLEN)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    os.makedirs(args.outdir, exist_ok=True)\n",
    "    df = load_df(args.input)\n",
    "    targets_all = build_targets(df)\n",
    "    seqs_all = build_sequences(df, args.seqlen)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    families = (targets_all[[\"Gender\",\"SeasonLabel\"]].drop_duplicates()\n",
    "                .sort_values([\"Gender\",\"SeasonLabel\"]))\n",
    "\n",
    "    family_metrics = []\n",
    "    for _, fam in families.iterrows():\n",
    "        gender, season = fam[\"Gender\"], fam[\"SeasonLabel\"]\n",
    "        F = targets_all[(targets_all[\"Gender\"]==gender) & (targets_all[\"SeasonLabel\"]==season)].copy()\n",
    "        if F.empty: continue\n",
    "\n",
    "        # Build aligned dataset\n",
    "        Xs, ys, years, events = [], [], [], []\n",
    "        for _, r in F.iterrows():\n",
    "            X = seqs_all.get((r[\"key\"], r[\"season_key\"]))\n",
    "            if X is None: continue\n",
    "            Xs.append(pad_seq(X, args.seqlen))\n",
    "            ys.append(float(r[\"y_next\"]))\n",
    "            years.append(int(r[\"SeasonYear\"]))\n",
    "            events.append(r[\"Event\"])\n",
    "        if not Xs: continue\n",
    "\n",
    "        X = np.stack(Xs); y = np.array(ys, dtype=\"float32\"); years = np.array(years); events = np.array(events)\n",
    "\n",
    "        # time-based split (fallback randomized if empty)\n",
    "        train_m = years <= 2023; val_m = years == 2024; test_m = years >= 2025\n",
    "        if not train_m.any():\n",
    "            idx = np.arange(len(X)); np.random.shuffle(idx)\n",
    "            n=len(idx); a=int(0.7*n); b=int(0.85*n)\n",
    "            train_m = np.zeros(n,bool); val_m=np.zeros(n,bool); test_m=np.zeros(n,bool)\n",
    "            train_m[idx[:a]]=True; val_m[idx[a:b]]=True; test_m[idx[b:]]=True\n",
    "\n",
    "        # include only events with enough TRAIN samples\n",
    "        tr_events, tr_counts = np.unique(events[train_m], return_counts=True)\n",
    "        allowed = set(e for e,c in zip(tr_events, tr_counts) if c >= args.min_train_per_event)\n",
    "        keep = np.array([e in allowed for e in events])\n",
    "        X, y, years, events = X[keep], y[keep], years[keep], events[keep]\n",
    "        train_m, val_m, test_m = train_m[keep], val_m[keep], test_m[keep]\n",
    "\n",
    "        if len(allowed) == 0:\n",
    "            print(f\"Skipping family {gender} | {season} (no event has ≥{args.min_train_per_event} train samples)\")\n",
    "            continue\n",
    "\n",
    "        evt_list = sorted(list(allowed))\n",
    "        evt2id = {e:i for i,e in enumerate(evt_list)}\n",
    "        evt_id = np.array([evt2id[e] for e in events], dtype=np.int64)\n",
    "\n",
    "        # per-event z-score (fit on TRAIN)\n",
    "        y_z = y.copy()\n",
    "        mu_by_evt, sd_by_evt = {}, {}\n",
    "        for e in evt_list:\n",
    "            idx = (evt_id==evt2id[e]) & train_m\n",
    "            mu = y[idx].mean() if idx.any() else 0.0\n",
    "            sd = y[idx].std() if idx.any() else 1.0\n",
    "            if sd < 1e-6: sd = 1.0\n",
    "            mu_by_evt[e], sd_by_evt[e] = float(mu), float(sd)\n",
    "            idx_all = (evt_id==evt2id[e])\n",
    "            y_z[idx_all] = (y[idx_all]-mu)/sd\n",
    "\n",
    "        # datasets/loaders\n",
    "        class FamDS(Dataset):\n",
    "            def __init__(self, X, eids, y):\n",
    "                self.X = torch.tensor(X, dtype=torch.float32)\n",
    "                self.e = torch.tensor(eids, dtype=torch.long)\n",
    "                self.y = torch.tensor(y, dtype=torch.float32)\n",
    "            def __len__(self): return len(self.X)\n",
    "            def __getitem__(self, i): return self.X[i], self.e[i], self.y[i]\n",
    "\n",
    "        train_dl = DataLoader(FamDS(X[train_m], evt_id[train_m], y_z[train_m]), batch_size=BATCH, shuffle=True)\n",
    "        val_dl   = DataLoader(FamDS(X[val_m], evt_id[val_m], y_z[val_m]), batch_size=256)\n",
    "        test_dl  = DataLoader(FamDS(X[test_m], evt_id[test_m], y_z[test_m]), batch_size=256)\n",
    "\n",
    "        # model\n",
    "        class SharedEncoderPerEventHeads(nn.Module):\n",
    "            def __init__(self, in_dim, hid, n_events):\n",
    "                super().__init__()\n",
    "                self.lstm = nn.LSTM(in_dim, hid, batch_first=True)\n",
    "                self.heads = nn.ModuleList([nn.Sequential(nn.LayerNorm(hid), nn.Linear(hid, 1))\n",
    "                                            for _ in range(n_events)])\n",
    "            def forward(self, x, evt_id):\n",
    "                out, _ = self.lstm(x)\n",
    "                last = out[:, -1, :]\n",
    "                preds = torch.empty(x.size(0), device=x.device, dtype=torch.float32)\n",
    "                for eid in torch.unique(evt_id):\n",
    "                    mask = (evt_id == eid)\n",
    "                    preds[mask] = self.heads[int(eid)](last[mask]).squeeze(-1)\n",
    "                return preds\n",
    "\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model = SharedEncoderPerEventHeads(in_dim=X.shape[-1], hid=args.hid, n_events=len(evt_list)).to(device)\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)\n",
    "        loss_fn = nn.L1Loss()\n",
    "\n",
    "        def run_epoch(dl, train=True):\n",
    "            model.train(train); tot=0; n=0\n",
    "            for xb, eb, yb in dl:\n",
    "                xb, eb, yb = xb.to(device), eb.to(device), yb.to(device)\n",
    "                with torch.set_grad_enabled(train):\n",
    "                    pred = model(xb, eb); loss = loss_fn(pred, yb)\n",
    "                if train:\n",
    "                    opt.zero_grad(); loss.backward(); opt.step()\n",
    "                tot += loss.item()*len(xb); n+=len(xb)\n",
    "            return tot/max(n,1)\n",
    "\n",
    "        best=1e9; patience=0; best_state=None\n",
    "        for ep in range(EPOCHS):\n",
    "            tr = run_epoch(train_dl, True)\n",
    "            va = run_epoch(val_dl, False)\n",
    "            print(f\"[FSMH-LSTM] {gender} | {season} ep {ep:02d}  train L1(z)={tr:.3f}  val L1(z)={va:.3f}\")\n",
    "            if va < best - 1e-3:\n",
    "                best, patience = va, 0\n",
    "                best_state = {k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "            else:\n",
    "                patience += 1\n",
    "            if patience >= 5: break\n",
    "        if best_state is not None: model.load_state_dict(best_state)\n",
    "\n",
    "        # evaluate on TEST in native units\n",
    "        model.eval()\n",
    "        preds_z = []\n",
    "        with torch.no_grad():\n",
    "            for xb, eb, yb in test_dl:\n",
    "                xb, eb = xb.to(device), eb.to(device)\n",
    "                preds_z.append(model(xb, eb).cpu().numpy())\n",
    "        preds_z = np.concatenate(preds_z) if preds_z else np.array([])\n",
    "        y_test = y[test_m]\n",
    "        e_test = evt_id[test_m]\n",
    "\n",
    "        # de-standardize per event\n",
    "        yhat = np.empty_like(preds_z)\n",
    "        for e, eid in ((e, i) for i,e in enumerate(evt_list)):\n",
    "            idx = (e_test==eid)\n",
    "            if idx.any():\n",
    "                mu, sd = mu_by_evt[e], sd_by_evt[e]\n",
    "                yhat[idx] = preds_z[idx]*sd + mu\n",
    "\n",
    "        overall_mae = float(np.mean(np.abs(yhat - y_test))) if len(yhat) else float(\"nan\")\n",
    "        # per-event MAE\n",
    "        per_event = []\n",
    "        for e, eid in ((e, i) for i,e in enumerate(evt_list)):\n",
    "            idx = (e_test==eid)\n",
    "            if idx.any():\n",
    "                mae = float(np.mean(np.abs(yhat[idx] - y_test[idx])))\n",
    "                per_event.append({\"event\":e, \"test_mae\":mae, \"n\":int(idx.sum())})\n",
    "\n",
    "        # save family model\n",
    "        fam_dir = os.path.join(args.outdir, f\"{safe_slug(gender)}__{safe_slug(season)}\")\n",
    "        os.makedirs(fam_dir, exist_ok=True)\n",
    "        torch.save(model.state_dict(), os.path.join(fam_dir,\"model.pth\"))\n",
    "        meta = {\n",
    "            \"model_name\":\"FSMH-LSTM\",\n",
    "            \"gender\":gender,\"season\":season,\n",
    "            \"events\":evt_list,\n",
    "            \"event2id\":evt2id,\n",
    "            \"mu_by_event\":mu_by_evt,\n",
    "            \"sd_by_event\":sd_by_evt,\n",
    "            \"hid\":args.hid,\"seqlen\":args.seqlen,\"in_dim\":X.shape[-1],\n",
    "            \"created\":datetime.utcnow().isoformat()+\"Z\",\n",
    "            \"val_L1_z\":float(best),\n",
    "            \"test_mae_overall\":overall_mae,\n",
    "            \"per_event_metrics\":per_event\n",
    "        }\n",
    "        with open(os.path.join(fam_dir,\"meta.json\"),\"w\",encoding=\"utf-8\") as f: json.dump(meta,f,indent=2)\n",
    "\n",
    "        # accumulate for global summary\n",
    "        row = {\"gender\":gender,\"season\":season,\"overall_test_mae\":overall_mae,\"n_events\":len(evt_list)}\n",
    "        for pe in per_event:\n",
    "            row[f\"MAE_{pe['event']}\"]=pe[\"test_mae\"]\n",
    "        family_metrics.append(row)\n",
    "\n",
    "    # write summary CSV\n",
    "    if family_metrics:\n",
    "        pd.DataFrame(family_metrics).to_csv(os.path.join(args.outdir,\"summary_metrics.csv\"), index=False)\n",
    "        print(f\"✅ Saved FSMH-LSTM families → {args.outdir}\")\n",
    "    else:\n",
    "        print(\"⚠️ No families trained (insufficient data with current thresholds).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
